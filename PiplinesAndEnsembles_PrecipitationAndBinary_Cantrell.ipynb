{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods and Pipelines\n",
    "\n",
    "Ensemble methods could make up the content of an entire course. **Ensembles combine more than one model systematically.** In practice, data scientists do this because it helps them predict outcomes with more accuracy and less bias when compared to any single method. Since there are thousands of possible algorithms that can be combined in what are basically limitless ways, there is no way to cover all possible ensemble methods. Instead, we will discuss one particular ensemble method called Random Forest, which is quite widely used. Note that the idea of combining multiple relatively \"weak\" machine learning methods into a single \"strong\" machine learning method is quite common, and that most sophisticated machine learning models do this. \n",
    "\n",
    "Also in this lesson, we will introduce the idea of a **\"pipeline\"**, which is a collection of steps that you can use to automate a lot of your data science process so that testing modifications and variation implementation are relatively straightforward/simple. Pipelines require some getting used to, but are worth the effort in the end. **similar to loops - looking at data specific hyper-parameters**\n",
    "* how do we pick number of folds for cross validation - w/ pipelines, can iterate through easier\n",
    "* can use grid search\n",
    "\n",
    "Rather than go through a lots of explanations, we're going to build an ensemble model and talk about what it does. Then we're going to do the same for a pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in packages/libraries\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')   # want to ignore the warnings for this assignment\n",
    "import numpy as np\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifiers\n",
    "\n",
    "```clf = LogisticRegression() ``` \n",
    "\n",
    "versus\n",
    "\n",
    "```clf = RandomForestClassifier()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precip</th>\n",
       "      <th>binary</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precip  binary  month  year\n",
       "0     6.7       1      1  1901\n",
       "1     0.0       0      1  1901\n",
       "2     1.7       0      1  1901\n",
       "3     3.8       1      1  1901\n",
       "4     6.3       1      1  1901"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "# source: https://data.gov.in/catalog/rainfall-india?filters%5Bfield_catalog_reference%5D=1090541&format=json&offset=0&limit=6&sort%5Bcreated%5D=desc\n",
    "import os\n",
    "os.chdir(r'C:\\\\Users\\\\livsh\\\\Downloads') \n",
    "transposed = pd.read_csv('binary_and_precip_transposed.csv')\n",
    "transposed = transposed.drop(transposed.columns[0], axis = 1)\n",
    "transposed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `binary_and_precip_transposed.csv` is from Lesson_8_Cantrell_Project_Application\n",
    "\n",
    "I added this cell of code to lesson 8: \n",
    " transposed.to_csv('binary_and_precip_transposed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `df_binary.csv` is from Lesson_8_Cantrell_Project Application\n",
    "I added this cell of code to lesson 8: \n",
    "df_binary.to_csv('df_binary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JAN_bi</th>\n",
       "      <th>FEB_bi</th>\n",
       "      <th>MAR_bi</th>\n",
       "      <th>APR_bi</th>\n",
       "      <th>MAY_bi</th>\n",
       "      <th>JUN_bi</th>\n",
       "      <th>JUL_bi</th>\n",
       "      <th>AUG_bi</th>\n",
       "      <th>SEP_bi</th>\n",
       "      <th>OCT_bi</th>\n",
       "      <th>NOV_bi</th>\n",
       "      <th>DEC_bi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   JAN_bi  FEB_bi  MAR_bi  APR_bi  MAY_bi  JUN_bi  JUL_bi  AUG_bi  SEP_bi  \\\n",
       "0       1       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       1       0       0       1   \n",
       "2       0       0       1       0       0       0       1       0       1   \n",
       "3       1       0       1       0       1       0       0       0       0   \n",
       "4       1       0       0       0       0       0       0       0       1   \n",
       "\n",
       "   OCT_bi  NOV_bi  DEC_bi  \n",
       "0       0       0       0  \n",
       "1       0       0       0  \n",
       "2       0       0       0  \n",
       "3       0       1       1  \n",
       "4       0       0       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_binary = pd.read_csv('df_binary.csv')\n",
    "df_binary = df_binary.drop(df_binary.columns[0], axis = 1)\n",
    "df_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 1399 1400 1402] TEST: [   4    5   19   27   31   34   45   52   54   55   85  108  141  142\n",
      "  148  152  159  161  184  192  202  211  224  227  231  233  241  247\n",
      "  254  268  278  298  303  308  312  326  362  363  376  412  418  420\n",
      "  426  438  445  458  461  467  471  472  477  487  500  526  528  529\n",
      "  533  536  542  554  557  563  565  568  569  572  587  608  609  610\n",
      "  634  638  642  649  654  656  678  704  708  711  717  740  757  758\n",
      "  759  761  762  768  788  792  795  798  826  846  877  887  901  911\n",
      "  918  920  922  935  946  963  980  983  986 1000 1002 1010 1024 1032\n",
      " 1034 1038 1041 1063 1070 1127 1150 1154 1168 1174 1179 1183 1222 1235\n",
      " 1252 1257 1259 1261 1270 1298 1299 1332 1347 1349 1372 1375 1396 1401\n",
      " 1403]\n",
      "TRAIN: [   0    3    4 ... 1401 1402 1403] TEST: [   1    2    8    9   14   18   29   39   40   47   53   56   58   61\n",
      "   75   80   92  124  140  156  182  186  198  204  215  253  260  270\n",
      "  279  295  299  302  310  315  317  319  322  330  333  361  381  390\n",
      "  393  399  406  408  411  427  431  432  442  464  465  466  481  491\n",
      "  494  506  520  540  546  548  549  574  580  583  589  597  613  621\n",
      "  632  644  679  686  692  725  751  760  764  771  772  775  781  783\n",
      "  789  790  805  829  840  853  869  883  884  892  896  898  902  905\n",
      "  907  937  939  959  974  977  987  996 1012 1023 1027 1042 1047 1058\n",
      " 1062 1072 1075 1080 1108 1114 1120 1165 1177 1182 1193 1197 1210 1213\n",
      " 1218 1248 1265 1272 1280 1281 1288 1318 1328 1333 1345 1348 1357 1371\n",
      " 1400]\n",
      "TRAIN: [   0    1    2 ... 1401 1402 1403] TEST: [  10   15   44   49   60   64   70   82  103  131  165  187  191  194\n",
      "  195  240  251  269  276  283  300  320  328  338  342  353  364  402\n",
      "  416  422  425  434  436  440  443  446  459  474  483  489  501  505\n",
      "  539  567  581  590  601  618  619  641  651  655  661  665  666  667\n",
      "  687  695  703  712  716  726  742  743  746  748  784  787  799  801\n",
      "  817  818  828  831  838  858  863  866  879  895  897  899  900  904\n",
      "  913  924  938  943  955  970 1013 1051 1059 1074 1079 1081 1093 1099\n",
      " 1103 1115 1118 1121 1125 1131 1136 1146 1149 1151 1170 1181 1190 1195\n",
      " 1202 1217 1229 1233 1237 1266 1271 1279 1282 1284 1286 1300 1303 1315\n",
      " 1321 1322 1324 1331 1335 1336 1343 1344 1346 1350 1363 1365 1376 1392\n",
      " 1393]\n",
      "TRAIN: [   0    1    2 ... 1401 1402 1403] TEST: [  16   65   77   81   88   97  110  113  117  122  175  183  188  196\n",
      "  200  239  249  252  259  261  263  265  267  271  272  294  311  349\n",
      "  351  356  358  359  367  371  384  386  395  397  405  435  452  453\n",
      "  473  478  479  482  486  503  511  512  513  517  530  535  541  553\n",
      "  575  578  579  615  629  657  674  682  688  721  737  744  752  785\n",
      "  810  811  830  839  841  857  912  915  916  923  927  934  936  940\n",
      "  942  953  957  961  962  981  991  993 1003 1017 1019 1021 1030 1031\n",
      " 1044 1054 1057 1101 1113 1126 1142 1157 1184 1187 1194 1200 1211 1219\n",
      " 1220 1224 1227 1234 1242 1243 1245 1253 1260 1262 1273 1278 1285 1296\n",
      " 1297 1305 1306 1325 1330 1339 1341 1359 1360 1362 1373 1374 1386 1387\n",
      " 1389]\n",
      "TRAIN: [   0    1    2 ... 1401 1402 1403] TEST: [   6   13   20   30   37   38   50   62   79   83   95  101  105  118\n",
      "  127  143  144  145  150  154  158  162  172  181  190  193  206  214\n",
      "  222  228  230  235  236  242  255  262  264  266  301  316  318  331\n",
      "  352  366  382  403  413  457  484  485  492  495  521  523  531  545\n",
      "  552  566  571  588  592  625  646  648  652  662  669  670  672  689\n",
      "  693  713  722  724  728  729  731  733  773  776  808  820  822  823\n",
      "  824  842  867  874  875  880  891  906  914  921  930  948  971  975\n",
      "  985  988  992  995 1015 1018 1026 1050 1052 1095 1098 1109 1123 1128\n",
      " 1132 1140 1156 1186 1191 1212 1226 1230 1232 1236 1241 1258 1268 1287\n",
      " 1293 1294 1302 1307 1311 1316 1323 1327 1329 1351 1353 1361 1378 1394]\n",
      "TRAIN: [   0    1    2 ... 1400 1401 1403] TEST: [  12   17   21   35   42   46   48   68   71   76   78   90  116  119\n",
      "  133  157  167  170  177  179  208  210  217  225  229  243  250  258\n",
      "  285  288  289  293  304  306  313  325  332  337  344  345  346  350\n",
      "  372  378  380  385  389  396  409  410  421  424  439  447  451  462\n",
      "  490  493  496  507  515  522  524  532  561  564  570  576  593  596\n",
      "  616  622  630  636  643  658  664  668  671  699  747  753  754  766\n",
      "  769  793  794  813  844  847  861  864  870  871  878  881  933  958\n",
      "  960  965  968  982  994  997 1005 1014 1025 1036 1039 1046 1049 1064\n",
      " 1069 1083 1088 1096 1110 1130 1135 1145 1147 1159 1178 1188 1189 1215\n",
      " 1239 1240 1256 1283 1308 1314 1326 1356 1369 1377 1385 1390 1395 1402]\n",
      "TRAIN: [   1    2    3 ... 1401 1402 1403] TEST: [   0   22   51   66   74   89   96  100  104  107  109  114  120  125\n",
      "  132  138  155  171  173  178  185  205  218  219  220  238  245  282\n",
      "  309  321  334  354  360  374  375  379  391  392  394  401  404  414\n",
      "  415  419  428  441  456  460  476  497  499  516  518  519  527  538\n",
      "  558  582  585  602  604  605  614  624  626  627  635  640  660  676\n",
      "  677  681  683  685  690  691  700  702  706  709  718  720  723  738\n",
      "  741  745  780  796  814  815  825  827  833  834  848  856  862  893\n",
      "  926  944  950  966  969  979  984  989  999 1001 1011 1045 1048 1061\n",
      " 1073 1076 1086 1087 1089 1092 1137 1139 1144 1158 1163 1173 1185 1192\n",
      " 1209 1225 1231 1238 1244 1247 1292 1295 1313 1317 1379 1382 1388 1397]\n",
      "TRAIN: [   0    1    2 ... 1401 1402 1403] TEST: [   3    7   26   59   63  102  112  126  134  135  137  147  153  160\n",
      "  163  213  223  226  237  246  248  280  281  284  292  305  327  336\n",
      "  339  343  347  348  355  357  365  369  400  407  417  437  449  454\n",
      "  455  463  468  470  475  502  504  534  550  573  577  584  586  595\n",
      "  603  612  620  631  647  650  653  680  694  698  707  715  732  736\n",
      "  750  786  800  819  832  836  837  851  852  854  855  873  890  903\n",
      "  929  932  941  945  947  951  990  998 1006 1008 1009 1016 1028 1029\n",
      " 1037 1055 1056 1078 1084 1090 1091 1097 1100 1102 1105 1111 1116 1122\n",
      " 1124 1129 1133 1148 1160 1161 1164 1166 1175 1180 1196 1203 1205 1223\n",
      " 1249 1254 1264 1267 1291 1309 1310 1320 1338 1352 1358 1367 1381 1384]\n",
      "TRAIN: [   0    1    2 ... 1401 1402 1403] TEST: [  24   25   33   36   41   43   67   69   73   93   94  106  111  121\n",
      "  129  136  139  146  149  164  166  168  176  180  189  197  199  207\n",
      "  212  216  221  232  234  274  287  290  296  297  323  329  340  341\n",
      "  429  433  450  469  480  498  508  509  514  525  547  556  591  598\n",
      "  606  607  617  623  628  645  663  673  696  701  710  727  730  735\n",
      "  749  765  767  803  804  807  809  812  821  843  859  860  865  876\n",
      "  885  886  889  894  919  925  928  949  952  956  967  978 1004 1007\n",
      " 1020 1022 1035 1040 1043 1053 1068 1085 1107 1112 1117 1134 1138 1141\n",
      " 1153 1162 1169 1176 1199 1214 1228 1246 1250 1255 1263 1269 1274 1275\n",
      " 1276 1277 1290 1301 1304 1334 1337 1340 1342 1355 1368 1370 1398 1399]\n",
      "TRAIN: [   0    1    2 ... 1401 1402 1403] TEST: [  11   23   28   32   57   72   84   86   87   91   98   99  115  123\n",
      "  128  130  151  169  174  201  203  209  244  256  257  273  275  277\n",
      "  286  291  307  314  324  335  368  370  373  377  383  387  388  398\n",
      "  423  430  444  448  488  510  537  543  544  551  555  559  560  562\n",
      "  594  599  600  611  633  637  639  659  675  684  697  705  714  719\n",
      "  734  739  755  756  763  770  774  777  778  779  782  791  797  802\n",
      "  806  816  835  845  849  850  868  872  882  888  908  909  910  917\n",
      "  931  954  964  972  973  976 1033 1060 1065 1066 1067 1071 1077 1082\n",
      " 1094 1104 1106 1119 1143 1152 1155 1167 1171 1172 1198 1201 1204 1206\n",
      " 1207 1208 1216 1221 1251 1289 1312 1319 1354 1364 1366 1380 1383 1391]\n"
     ]
    }
   ],
   "source": [
    "# splitting up data\n",
    "# splits into roughly equally sized \n",
    "# shuffle - shuffles before splitting the data\n",
    "\n",
    "# random state - the random way we are selecting is going to be the same everytime so it is repeatable, a seed and how we do the shuffle\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)  # making 10 fold - each are going to have 10% of the data\n",
    "for train_index, test_index in cv.split(transposed):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    \n",
    "# the indicies in train should not be in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 1263 TEST: 141\n",
      "TRAIN: 1263 TEST: 141\n",
      "TRAIN: 1263 TEST: 141\n",
      "TRAIN: 1263 TEST: 141\n",
      "TRAIN: 1264 TEST: 140\n",
      "TRAIN: 1264 TEST: 140\n",
      "TRAIN: 1264 TEST: 140\n",
      "TRAIN: 1264 TEST: 140\n",
      "TRAIN: 1264 TEST: 140\n",
      "TRAIN: 1264 TEST: 140\n"
     ]
    }
   ],
   "source": [
    "# looking at the size of the test and train\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "for train_index, test_index in cv.split(transposed):\n",
    "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score \n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precip</th>\n",
       "      <th>binary</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precip  binary  month  year\n",
       "0     6.7       1      1  1901\n",
       "1     0.0       0      1  1901\n",
       "2     1.7       0      1  1901\n",
       "3     3.8       1      1  1901\n",
       "4     6.3       1      1  1901"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transposed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  52.4\n",
      "Precision:  66.7\n",
      "Precision:  71.4\n",
      "Precision:  70.8\n",
      "Precision:  80.8\n",
      "Precision:  52.2\n",
      "Precision:  55.6\n",
      "Precision:  54.5\n",
      "Precision:  76.0\n",
      "Precision:  75.0\n"
     ]
    }
   ],
   "source": [
    "# Define function\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=None)\n",
    "\n",
    "# Create for-loop\n",
    "for train_index, test_index in cv.split(transposed):\n",
    "\n",
    "    # Define training and test sets\n",
    "    X_train = transposed.loc[train_index].drop(['year', 'month', 'binary'], axis=1)\n",
    "    y_train = transposed.loc[train_index]['binary']    # what we want to predict\n",
    "    X_test = transposed.loc[test_index].drop(['year', 'month', 'binary'], axis=1)\n",
    "    y_test = transposed.loc[test_index]['binary']\n",
    "    \n",
    "        \n",
    "    # Fit model\n",
    "    clf = LogisticRegression(max_iter = 10000)  # model operator based on logistic regression, set max iterations\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Generate predictions\n",
    "    predicted = clf.predict(X_test)  # predicted model and output using this subset\n",
    "    \n",
    "    # Compare to actual outcomes and return precision (dont want a really long number so round)\n",
    "    print('Precision: ', (round(precision_score(y_test, predicted)*100,1))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  59.5\n",
      "Precision:  55.3\n",
      "Precision:  68.8\n",
      "Precision:  62.2\n",
      "Precision:  57.5\n",
      "Precision:  67.4\n",
      "Precision:  68.0\n",
      "Precision:  71.1\n",
      "Precision:  56.8\n",
      "Precision:  74.4\n"
     ]
    }
   ],
   "source": [
    "# Define function\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# Create for-loop\n",
    "for train_index, test_index in cv.split(transposed):\n",
    "\n",
    "    # Define training and test sets\n",
    "    X_train = transposed.loc[train_index].drop(['year', 'month', 'binary'], axis=1)\n",
    "    y_train = transposed.loc[train_index]['binary']    # what we want to predict\n",
    "    X_test = transposed.loc[test_index].drop(['year', 'month', 'binary'], axis=1)\n",
    "    y_test = transposed.loc[test_index]['binary']\n",
    "    \n",
    "        \n",
    "    # Fit model\n",
    "    clf = RandomForestClassifier(random_state=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Generate predictions\n",
    "    predicted = clf.predict(X_test)  # predicted model and output using this subset\n",
    "    \n",
    "    # Compare to actual outcomes and return precision (dont want a really long number so round)\n",
    "    print('Precision: ', (round(precision_score(y_test, predicted)*100,1))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adjust Parameters for Random Forest Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  59.5\n",
      "Precision:  55.3\n",
      "Precision:  68.8\n",
      "Precision:  62.2\n",
      "Precision:  57.5\n",
      "Precision:  67.4\n",
      "Precision:  68.0\n",
      "Precision:  71.1\n",
      "Precision:  56.8\n",
      "Precision:  74.4\n"
     ]
    }
   ],
   "source": [
    "# Define function\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# Create for-loop\n",
    "for train_index, test_index in cv.split(transposed):\n",
    "\n",
    "    # Define training and test sets\n",
    "    X_train = transposed.loc[train_index].drop(['year', 'month', 'binary'], axis=1)\n",
    "    y_train = transposed.loc[train_index]['binary']    # what we want to predict\n",
    "    X_test = transposed.loc[test_index].drop(['year', 'month', 'binary'], axis=1)\n",
    "    y_test = transposed.loc[test_index]['binary']\n",
    "    \n",
    "        \n",
    "    # Fit model\n",
    "    clf = RandomForestClassifier(random_state=1, oob_score = True)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Generate predictions\n",
    "    predicted = clf.predict(X_test)  # predicted model and output using this subset\n",
    "    \n",
    "    # Compare to actual outcomes and return precision (dont want a really long number so round)\n",
    "    print('Precision: ', (round(precision_score(y_test, predicted)*100,1))) \n",
    "    \n",
    "    \n",
    "# oob_score = True did not change the precision values compared to the random forest classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  55.6\n",
      "Precision:  59.5\n",
      "Precision:  71.7\n",
      "Precision:  57.5\n",
      "Precision:  59.5\n",
      "Precision:  66.0\n",
      "Precision:  66.7\n",
      "Precision:  68.1\n",
      "Precision:  60.0\n",
      "Precision:  73.3\n"
     ]
    }
   ],
   "source": [
    "# Define function\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# Create for-loop\n",
    "for train_index, test_index in cv.split(transposed):\n",
    "\n",
    "    # Define training and test sets\n",
    "    X_train = transposed.loc[train_index].drop(['year', 'month', 'binary'], axis=1)\n",
    "    y_train = transposed.loc[train_index]['binary']    # what we want to predict\n",
    "    X_test = transposed.loc[test_index].drop(['year', 'month', 'binary'], axis=1)\n",
    "    y_test = transposed.loc[test_index]['binary']\n",
    "    \n",
    "        \n",
    "    # Fit model\n",
    "    clf = RandomForestClassifier(random_state=1, n_estimators=100)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Generate predictions\n",
    "    predicted = clf.predict(X_test)  # predicted model and output using this subset\n",
    "    \n",
    "    # Compare to actual outcomes and return precision (dont want a really long number so round)\n",
    "    print('Precision: ', (round(precision_score(y_test, predicted)*100,1))) \n",
    "    \n",
    "    \n",
    "# N_estimators = 100 --> did not change the precison values compared to the random forest classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "Pipelines help you automate some of your work and make the process a bit more systematic. As you can likely tell, even though you aren't a software engineer, machine learning is all about letting machines do the work of \"learning\".  \n",
    "\n",
    "That's what pipelines allow us to do. You build a pipeline, tell the computer to use that pipeline to run many combinations of different features/parameters, and the machine tells you what works best.   \n",
    "\n",
    "Pipelines can do two things: \n",
    "1.) Transform data (tyically for feature engineering)\n",
    "2.) Estimate with the data (predicting outcomes given some data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search & Parameter Tuning\n",
    "\n",
    "Note that where pipelines really shine is in tuning hyperparameters. We can do this with for loops, but pipelines make it much easier.  \n",
    "\n",
    "\n",
    "**What is a hyperparameter?** https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/\n",
    "\n",
    "For example, with Random Forest Classification, we might want to adjust parameters like the number of estimators, or the minimum number of samples. Each of these are configurable by using sklearn's pipeline and grid search tools. The only trick is that we set these by using the model name, then two underscores, then the parameter name. So instead of using:\n",
    "\n",
    "```\n",
    "random_forest.n_estimators  \n",
    "```\n",
    "\n",
    "we should use:   *2 underscores and no .*\n",
    "\n",
    "```\n",
    "random_forest__n_estimators\n",
    "```\n",
    "\n",
    "We follow that with a list of the values we want to try. So, if we wanted to try all the values between 5 and 10 we could use either: \n",
    "\n",
    "``` \n",
    "random_forest__n_estimators=[5,6,7,8,9, 10]\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "``` \n",
    "random_forest__n_estimators=list(range(5,11))\n",
    "```\n",
    "\n",
    "which produces the same thing. If we think that trying every value will take too long (note, every additional variation is multiplied by all the other variations!), then maybe just try a few.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search over a few parameters using your precipitation data and binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87       247\n",
      "           1       0.70      0.63      0.67       104\n",
      "\n",
      "    accuracy                           0.81       351\n",
      "   macro avg       0.78      0.76      0.77       351\n",
      "weighted avg       0.81      0.81      0.81       351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.pipeline\n",
    "import sklearn.feature_selection\n",
    "\n",
    "X = transposed.drop(['binary', 'year', 'month'], axis=1).values  # independent variable \n",
    "y = transposed['binary'].values                 # dependent variable                          \n",
    "\n",
    "select = sklearn.feature_selection.SelectKBest(k='all')\n",
    "clf = sklearn.ensemble.RandomForestClassifier(random_state=1)\n",
    "\n",
    "steps = [('feature_selection', select),\n",
    "         ('random_forest', clf)]\n",
    "\n",
    "pipeline = sklearn.pipeline.Pipeline(steps) \n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1)  # splitting the data\n",
    "\n",
    "pipeline.fit( X_train, y_train ) # fit your pipeline on X_train and y_train\n",
    "y_prediction = pipeline.predict( X_test ) # call pipeline.predict() on your X_test data to make a set of test predictions\n",
    "report = sklearn.metrics.classification_report( y_test, y_prediction ) # test your predictions using sklearn.classification_report()\n",
    "print(report) # and print the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadening Your Horizons\n",
    "\n",
    "We don't have time to cover all the possible classifiers you can possibly use, but in the following code, we'll run through a few that you might find useful: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors 0.8042704626334519\n",
      "Nearest Neighbors 0.7437722419928826\n",
      "Nearest Neighbors 0.800711743772242\n",
      "Nearest Neighbors 0.7580071174377224\n",
      "Nearest Neighbors 0.7714285714285715\n",
      "Linear SVM 0.8113879003558719\n",
      "Linear SVM 0.7793594306049823\n",
      "Linear SVM 0.7935943060498221\n",
      "Linear SVM 0.8256227758007118\n",
      "Linear SVM 0.7535714285714286\n",
      "Gaussian Process 0.8185053380782918\n",
      "Gaussian Process 0.7793594306049823\n",
      "Gaussian Process 0.797153024911032\n",
      "Gaussian Process 0.8256227758007118\n",
      "Gaussian Process 0.7571428571428571\n",
      "Decision Tree 0.8042704626334519\n",
      "Decision Tree 0.7686832740213523\n",
      "Decision Tree 0.7829181494661922\n",
      "Decision Tree 0.8291814946619217\n",
      "Decision Tree 0.8071428571428572\n",
      "Random Forest 0.7935943060498221\n",
      "Random Forest 0.7864768683274022\n",
      "Random Forest 0.7900355871886121\n",
      "Random Forest 0.7330960854092526\n",
      "Random Forest 0.7392857142857143\n",
      "Neural Net 0.7295373665480427\n",
      "Neural Net 0.7580071174377224\n",
      "Neural Net 0.7829181494661922\n",
      "Neural Net 0.701067615658363\n",
      "Neural Net 0.7642857142857142\n",
      "AdaBoost 0.8113879003558719\n",
      "AdaBoost 0.7793594306049823\n",
      "AdaBoost 0.8042704626334519\n",
      "AdaBoost 0.8185053380782918\n",
      "AdaBoost 0.7714285714285715\n",
      "Naive Bayes 0.7259786476868327\n",
      "Naive Bayes 0.7402135231316725\n",
      "Naive Bayes 0.7722419928825622\n",
      "Naive Bayes 0.7295373665480427\n",
      "Naive Bayes 0.7357142857142858\n",
      "QDA 0.7259786476868327\n",
      "QDA 0.7402135231316725\n",
      "QDA 0.7722419928825622\n",
      "QDA 0.7295373665480427\n",
      "QDA 0.7357142857142858\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier  # there are classifiers and regressors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    SVC(),\n",
    "    GaussianProcessClassifier(),  # looking at normality\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(),\n",
    "    MLPClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=1) # not interating through the number of splits\n",
    "    for train_index, test_index in cv.split(transposed):\n",
    "        X_train = transposed.loc[train_index].drop(['year', 'month', 'binary'], axis=1)\n",
    "        y_train = transposed.loc[train_index]['binary']\n",
    "        X_test = transposed.loc[test_index].drop(['year', 'month', 'binary'], axis=1)\n",
    "        y_test = transposed.loc[test_index]['binary']\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "* Did not change the regressors because we the binary values are not continuous data\n",
    "* the various regressors are are similar in range\n",
    "* the \"worst\" regressors with the lowest values are QDA (similar to the inclass example) and Naive Byes\n",
    "* using literature review to help develop models for our projcect (looking at time series for next week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
